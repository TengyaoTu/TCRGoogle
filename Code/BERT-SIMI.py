#%%
import pandas as pd
import torch
from transformers import AutoTokenizer, AutoModel
from sklearn.metrics.pairwise import cosine_similarity
from tqdm import tqdm
import random
import numpy as np

# === Step 1: è¯»å–æ•°æ® ===
vdjdb_path = r"C:\Users\æ•°å­¦è¿·è¿·è¿·\Desktop\è›‹ç™½è´¨\vdjdb_filtered.csv"
trusted_path = r"C:\Users\æ•°å­¦è¿·è¿·è¿·\Desktop\è›‹ç™½è´¨\protgpt2_finetune_data.csv"
vdjdb_df = pd.read_csv(vdjdb_path)
trusted_df = pd.read_csv(trusted_path)

# æå–éœ€è¦åˆ†ç±»çš„æœªçŸ¥ TCR åºåˆ—ï¼ˆscore = 0ï¼‰
cdr3_unknown = vdjdb_df[vdjdb_df["vdjdb.score"] == 0]["cdr3"].dropna().astype(str).tolist()
score0_idx = vdjdb_df[vdjdb_df["vdjdb.score"] == 0].index

# ä»å¯ä¿¡æ•°æ®ä¸­æŠ½å– 20000 æ¡åºåˆ—
trusted_seqs = trusted_df["Sequence"].dropna().astype(str).tolist()
trusted_cdr3 = [s.split("XXX")[0] for s in trusted_seqs]
random.seed(42)
trusted_sample = random.sample(trusted_cdr3, 20000)

# === Step 2: åŠ è½½ TCR-BERT æ¨¡å‹ ===
model_name = "wukevin/tcr-bert"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModel.from_pretrained(model_name).eval().to("cuda" if torch.cuda.is_available() else "cpu")
device = model.device

def encode(seq):
    inputs = tokenizer(seq, return_tensors="pt", truncation=True, padding=True)
    inputs = {k: v.to(device) for k, v in inputs.items()}
    with torch.no_grad():
        out = model(**inputs)
    return out.last_hidden_state[:, 0, :].squeeze().cpu()

# === Step 3: æå–åµŒå…¥ ===
print("ğŸš€ æå– 20000 æ¡å¯ä¿¡ TCR åµŒå…¥...")
trusted_vecs = [encode(s) for s in tqdm(trusted_sample)]
trusted_tensor = torch.stack(trusted_vecs)

print("ğŸ” æå– 337 æ¡æœªçŸ¥ TCR åµŒå…¥...")
unknown_vecs = [encode(s) for s in tqdm(cdr3_unknown)]
unknown_tensor = torch.stack(unknown_vecs)

# === Step 4: è®¡ç®—æœ€å¤§åŒ¹é…ç›¸ä¼¼åº¦ ===
print("ğŸ“ è®¡ç®—æœ€å¤§åŒ¹é…ä½™å¼¦ç›¸ä¼¼åº¦...")
sim_matrix = cosine_similarity(unknown_tensor, trusted_tensor)
match_sim = sim_matrix.max(axis=1)  # âœ… ä½ è¦çš„æœ€å¤§å€¼ç­–ç•¥ï¼

# === Step 5: ä¼ªæ ‡ç­¾åˆ’åˆ†ï¼ˆæŒ‰ match_sim æ’åºå¯¹åŠåˆ‡ï¼‰ ===
sorted_idx = np.argsort(match_sim)
half = len(match_sim) // 2
pseudo = np.ones(len(match_sim), dtype=int)
pseudo[sorted_idx[:half]] = 0

# æ’å…¥ç»“æœ
vdjdb_df["pseudo_class"] = 2  # é»˜è®¤å¯ä¿¡
vdjdb_df.loc[score0_idx, "trust_score"] = match_sim
vdjdb_df.loc[score0_idx, "pseudo_class"] = pseudo

# è¾“å‡ºé˜ˆå€¼
threshold_value = np.sort(match_sim)[half]
print(f"ğŸ“ æœ€å¤§åŒ¹é…ç›¸ä¼¼åº¦åˆ’åˆ†é˜ˆå€¼ä¸ºï¼š{threshold_value:.4f}")

# === Step 6: ä¿å­˜è¾“å‡º ===
output_path = r"C:\Users\æ•°å­¦è¿·è¿·è¿·\Desktop\è›‹ç™½è´¨\vdjdb_filtered_scored_20k_maxmatch.csv"
vdjdb_df.to_csv(output_path, index=False)
print(f"âœ… å·²ä¿å­˜åˆ†ç±»ç»“æœåˆ°ï¼š{output_path}")
import pandas as pd
import torch
from transformers import AutoTokenizer, AutoModel
from sklearn.metrics.pairwise import cosine_similarity
from tqdm import tqdm
import random
import numpy as np

# === Step 1: è¯»å–æ•°æ® ===
vdjdb_path = r"C:\Users\æ•°å­¦è¿·è¿·è¿·\Desktop\è›‹ç™½è´¨\vdjdb_filtered.csv"
trusted_path = r"C:\Users\æ•°å­¦è¿·è¿·è¿·\Desktop\è›‹ç™½è´¨\protgpt2_finetune_data.csv"
vdjdb_df = pd.read_csv(vdjdb_path)
trusted_df = pd.read_csv(trusted_path)

# æå– score=0 çš„ TCR åºåˆ—
cdr3_unknown = vdjdb_df[vdjdb_df["vdjdb.score"] == 0]["cdr3"].dropna().astype(str).tolist()
score0_idx = vdjdb_df[vdjdb_df["vdjdb.score"] == 0].index

# æŠ½å– 20000 æ¡å¯ä¿¡æ ·æœ¬ï¼ˆProtGPT2ï¼‰
trusted_seqs = trusted_df["Sequence"].dropna().astype(str).tolist()
trusted_cdr3 = [s.split("XXX")[0] for s in trusted_seqs]
random.seed(42)
trusted_sample = random.sample(trusted_cdr3, 20000)

# === Step 2: åŠ è½½æ¨¡å‹ ===
model_name = "wukevin/tcr-bert"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModel.from_pretrained(model_name).eval().to("cuda" if torch.cuda.is_available() else "cpu")
device = model.device

def encode(seq):
    inputs = tokenizer(seq, return_tensors="pt", truncation=True, padding=True)
    inputs = {k: v.to(device) for k, v in inputs.items()}
    with torch.no_grad():
        out = model(**inputs)
    return out.last_hidden_state[:, 0, :].squeeze().cpu()

# === Step 3: æå–åµŒå…¥ ===
print("ğŸš€ æå– 20000 æ¡å¯ä¿¡ TCR åµŒå…¥...")
trusted_vecs = [encode(s) for s in tqdm(trusted_sample)]
trusted_tensor = torch.stack(trusted_vecs)

print("ğŸ” æå– 337 æ¡æœªçŸ¥ TCR åµŒå…¥...")
unknown_vecs = [encode(s) for s in tqdm(cdr3_unknown)]
unknown_tensor = torch.stack(unknown_vecs)

# === Step 4: è®¡ç®—æœ€å¤§åŒ¹é…ç›¸ä¼¼åº¦ ===
print("ğŸ“ è®¡ç®—æœ€å¤§ä½™å¼¦ç›¸ä¼¼åº¦...")
sim_matrix = cosine_similarity(unknown_tensor, trusted_tensor)
max_sim = sim_matrix.max(axis=1)

# æ’å…¥æ‰“åˆ†ï¼ˆä¸åˆ†ç±»ï¼‰
vdjdb_df.loc[score0_idx, "trust_score"] = max_sim

# æ‰“å°æœ€é«˜ã€æœ€ä½ã€å¹³å‡å€¼ä¾›ä½ å‚è€ƒ
print(f"âœ… æœ€å¤§åŒ¹é…ç›¸ä¼¼åº¦ç»Ÿè®¡ï¼š")
print(f"ğŸ”¹ max: {np.max(max_sim):.4f}")
print(f"ğŸ”¹ min: {np.min(max_sim):.4f}")
print(f"ğŸ”¹ mean: {np.mean(max_sim):.4f}")
print(f"ğŸ”¹ median: {np.median(max_sim):.4f}")

# === Step 5: ä¿å­˜è¾“å‡º ===
output_path = r"C:\Users\æ•°å­¦è¿·è¿·è¿·\Desktop\è›‹ç™½è´¨\vdjdb_filtered_maxscore_only.csv"
vdjdb_df.to_csv(output_path, index=False)
print(f"âœ… å·²ä¿å­˜æœ€å¤§åŒ¹é…ç»“æœåˆ°ï¼š{output_path}")
import pandas as pd
import torch
from transformers import AutoTokenizer, AutoModel
from sklearn.metrics.pairwise import cosine_similarity
from tqdm import tqdm
import random
import numpy as np

# === Step 1: è¯»å–æ•°æ® ===
vdjdb_path = r"C:\Users\æ•°å­¦è¿·è¿·è¿·\Desktop\è›‹ç™½è´¨\vdjdb_filtered.csv"
trusted_path = r"C:\Users\æ•°å­¦è¿·è¿·è¿·\Desktop\è›‹ç™½è´¨\protgpt2_finetune_data.csv"
vdjdb_df = pd.read_csv(vdjdb_path)
trusted_df = pd.read_csv(trusted_path)

# æå– score=0 çš„ TCR åºåˆ—
cdr3_unknown = vdjdb_df[vdjdb_df["vdjdb.score"] == 0]["cdr3"].dropna().astype(str).tolist()
score0_idx = vdjdb_df[vdjdb_df["vdjdb.score"] == 0].index

# æŠ½å– 20000 æ¡å¯ä¿¡æ ·æœ¬ï¼ˆProtGPT2ï¼‰
trusted_seqs = trusted_df["Sequence"].dropna().astype(str).tolist()
trusted_cdr3 = [s.split("XXX")[0] for s in trusted_seqs]
random.seed(42)
trusted_sample = random.sample(trusted_cdr3, 20000)

# === Step 2: åŠ è½½æ¨¡å‹ ===
model_name = "wukevin/tcr-bert"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModel.from_pretrained(model_name).eval().to("cuda" if torch.cuda.is_available() else "cpu")
device = model.device

def encode(seq):
    inputs = tokenizer(seq, return_tensors="pt", truncation=True, padding=True)
    inputs = {k: v.to(device) for k, v in inputs.items()}
    with torch.no_grad():
        out = model(**inputs)
    return out.last_hidden_state[:, 0, :].squeeze().cpu()

# === Step 3: æå–åµŒå…¥ ===
print("ğŸš€ æå– 20000 æ¡å¯ä¿¡ TCR åµŒå…¥...")
trusted_vecs = [encode(s) for s in tqdm(trusted_sample)]
trusted_tensor = torch.stack(trusted_vecs)

print("ğŸ” æå– 337 æ¡æœªçŸ¥ TCR åµŒå…¥...")
unknown_vecs = [encode(s) for s in tqdm(cdr3_unknown)]
unknown_tensor = torch.stack(unknown_vecs)

# === Step 4: è®¡ç®—æœ€å¤§åŒ¹é…ç›¸ä¼¼åº¦ ===
print("ğŸ“ è®¡ç®—æœ€å¤§ä½™å¼¦ç›¸ä¼¼åº¦...")
sim_matrix = cosine_similarity(unknown_tensor, trusted_tensor)
max_sim = sim_matrix.max(axis=1)

# æ’å…¥æ‰“åˆ†ï¼ˆä¸åˆ†ç±»ï¼‰
vdjdb_df.loc[score0_idx, "trust_score"] = max_sim

# æ‰“å°æœ€é«˜ã€æœ€ä½ã€å¹³å‡å€¼ä¾›ä½ å‚è€ƒ
print(f"âœ… æœ€å¤§åŒ¹é…ç›¸ä¼¼åº¦ç»Ÿè®¡ï¼š")
print(f"ğŸ”¹ max: {np.max(max_sim):.4f}")
print(f"ğŸ”¹ min: {np.min(max_sim):.4f}")
print(f"ğŸ”¹ mean: {np.mean(max_sim):.4f}")
print(f"ğŸ”¹ median: {np.median(max_sim):.4f}")

# === Step 5: ä¿å­˜è¾“å‡º ===
output_path = r"C:\Users\æ•°å­¦è¿·è¿·è¿·\Desktop\è›‹ç™½è´¨\vdjdb_filtered_maxscore_only.csv"
vdjdb_df.to_csv(output_path, index=False)
print(f"âœ… å·²ä¿å­˜æœ€å¤§åŒ¹é…ç»“æœåˆ°ï¼š{output_path}")
